{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Классификация текстов по темам**\n",
    "\n",
    "Имеется датасет текстов постов из социальной сети (7500 в обучающей выборке и 2500 в тестовой). Их необходимо классифицировать по 13 темам, к которым они относятся.\n",
    "\n",
    "Для получения наилучшей метрики `balanced_accuracy_score` будет реализован стекинг базовых моделей:\n",
    "\n",
    "- извлечение эмбеддингов при помощи больших языковых моделей из `HuggingFace` (`sberbank-ai/ruBert-base`, `cointegrated/rubert-tiny2`, `DeepPavlov/rubert-base-cased-conversational`, `sentence-transformers/LaBSE`) и подача их вместе с текстом в `CatBoostClassifier`;\n",
    "- `TF-IDF` + классификаторы (`LogisticRegression`, `SVC`, `LinearSVC`, `ExtraTreesClassifier`).\n",
    "\n",
    "Финальное предсказание выполняет метамодель `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers optuna optuna-integration catboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbeddings:\n",
    "    def __init__(self, add_cls_embeddings=True, add_mean_embeddings=False):\n",
    "        self.add_mean_embeddings = add_mean_embeddings\n",
    "        self.add_cls_embeddings = add_cls_embeddings\n",
    "        if add_cls_embeddings is False and add_mean_embeddings is False:\n",
    "            raise 'Error: you should select at least one type of embeddings to be computed'\n",
    "\n",
    "    def mean_pooling(self, hidden_state, attention_mask):\n",
    "        \"\"\"\n",
    "        Возвращает усредненный с учетом attention_mask hidden_state.\n",
    "        \"\"\"\n",
    "        token_embeddings = hidden_state.detach().cpu() \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        return sum_embeddings / attention_mask.sum()\n",
    "\n",
    "    def extract_embeddings(self, texts, model_name, max_len):\n",
    "        \"\"\"\n",
    "        Возвращает значения, посчитанные данной моделью - эмбеддинги для всех текстов из texts.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).cuda()\n",
    "        text_features = []\n",
    "        for sentence in tqdm(texts):\n",
    "            encoded_input = tokenizer([sentence],\n",
    "                                      padding='max_length',\n",
    "                                      truncation=True,\n",
    "                                      max_length=max_len,\n",
    "                                      return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                hidden_state, cls_head = model(input_ids=encoded_input['input_ids'].cuda(), return_dict=False)\n",
    "                sentence_embeddings = self.mean_pooling(hidden_state, encoded_input['attention_mask'])\n",
    "            \n",
    "            now_emb = []\n",
    "            if self.add_cls_embeddings:\n",
    "                now_emb.append(cls_head.detach().cpu().numpy().flatten())\n",
    "            \n",
    "            if self.add_mean_embeddings:\n",
    "                now_emb.append(sentence_embeddings.detach().cpu().numpy().flatten())\n",
    "            \n",
    "            text_features.append(np.concatenate(now_emb, axis=0))\n",
    "        return text_features\n",
    "\n",
    "    def add_many_embeddings(self, df, text_col, models):\n",
    "        \"\"\"\"\n",
    "        Добавляет в качестве признаков эмбеддинги для колонки text_col.\n",
    "        В качестве моделей и максимальных длин используются models.\n",
    "        \"\"\"\n",
    "        for model_name, max_len in models:\n",
    "            print(model_name)\n",
    "            text_features = self.extract_embeddings(df[text_col], model_name, max_len)\n",
    "            text_features_df = pd.DataFrame(text_features, columns = [f'{model_name}_{text_col}_feature_{i}' for i in range(len(text_features[0]))])\n",
    "            df = df.join(text_features_df)\n",
    "            df.to_csv('transformers_text_features.csv', index=False)\n",
    "            os.system('cp /content/transformers_text_features.csv /content/drive/MyDrive/datasets/transformers_text_features.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(algorithm,\n",
    "                X,\n",
    "                y,\n",
    "                early_stopping_rounds,\n",
    "                init_params=None,\n",
    "                cat_features=None,\n",
    "                text_features=None,\n",
    "                random_seed=2024\n",
    "    ):\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    print(f\"========= TRAINING {algorithm.__name__} =========\")\n",
    "\n",
    "    for num_fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        if init_params is not None:\n",
    "            model = algorithm(**init_params)\n",
    "        else:\n",
    "            model = algorithm()\n",
    "\n",
    "        if 'CatBoost' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения \n",
    "            train_dataset = Pool(data=X_train, label=y_train, cat_features=cat_features, text_features=text_features)\n",
    "            eval_dataset  = Pool(data=X_eval, label=y_eval, cat_features=cat_features, text_features=text_features)\n",
    "\n",
    "            model.fit(train_dataset,\n",
    "                      eval_set=eval_dataset,\n",
    "                      verbose=0,\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        elif 'LGBM' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения \n",
    "            train_dataset = Dataset(X_train, y_train, categorical_feature=cat_features, free_raw_data=False,)\n",
    "            eval_dataset  = Dataset(X_eval, y_eval, categorical_feature=cat_features, free_raw_data=False,)\n",
    "\n",
    "            model = lgb.train(params=init_params,\n",
    "                              train_set=train_dataset,\n",
    "                              valid_sets=(eval_dataset),\n",
    "                              #callbacks=[lgb.log_evaluation(10)],\n",
    "                              #           lgb.early_stopping(stopping_rounds=5)],\n",
    "                              categorical_feature=cat_features,\n",
    "                              #verbose_eval=False                   # в новой версии LightGBM по логи по умолчанию отключены\n",
    "                              )\n",
    "\n",
    "        elif 'XGB' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения\n",
    "            train_dataset = xgb.DMatrix(X_train, label=y_train, nthread=-1, enable_categorical=True,)\n",
    "            eval_dataset  = xgb.DMatrix(X_eval,  label=y_eval,  nthread=-1, enable_categorical=True,)\n",
    "\n",
    "            model = xgb.train(params=init_params,\n",
    "                              dtrain=train_dataset,\n",
    "                              evals=[(train_dataset, 'dtrain'), (eval_dataset, 'dtest')],\n",
    "                              verbose_eval=False,\n",
    "                              early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "            X_eval = eval_dataset\n",
    "\n",
    "        # Предсказание на X_eval и расчет RMSE\n",
    "        y_pred = model.predict(X_eval)\n",
    "        score = balanced_accuracy_score(y_eval, y_pred)\n",
    "\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f'FOLD {num_fold}: SCORE {score}')\n",
    "\n",
    "    mean_kfold_score = np.mean(scores, dtype=\"float16\") - np.std(scores, dtype=\"float16\")\n",
    "    print(\"\\nMEAN BALANCED ACCURACY SCORE\", mean_kfold_score)\n",
    "\n",
    "    # Модель с наименьшим значением скора\n",
    "    best_model = models[scores.index(min(scores))]\n",
    "\n",
    "    return mean_kfold_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если файлы с эмбеддингами уже сформированы - загружаем их\n",
    "try:\n",
    "    pass\n",
    "# Если нет - загружаем исходные датасеты и извлекаем для них эмбеддинги\n",
    "except:\n",
    "    train = pd.read_csv('../data/text_classification_train.csv')\n",
    "    test = pd.read_csv('../data/text_classification_test.csv')\n",
    "\n",
    "    # Полный список поддерживаемых моделей можно найти на https://huggingface.co/models\n",
    "    models = [('sberbank-ai/ruBert-base', 512),\n",
    "              ('cointegrated/rubert-tiny2', 2048),\n",
    "              ('DeepPavlov/rubert-base-cased-conversational', 512),\n",
    "              ('sentence-transformers/LaBSE', 512),\n",
    "              ]\n",
    "\n",
    "    text_embeddings = TextEmbeddings(False, True)\n",
    "    train = text_embeddings.add_many_embeddings(train, 'text', models)\n",
    "    test = text_embeddings.add_many_embeddings(test, 'text', models)\n",
    "\n",
    "    train.to_csv('../tmp_data/train_with_embs.csv', index=False)\n",
    "    test.to_csv('../tmp_data/test_with_embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_STATE = 42\n",
    "cb_init_params_cust = {\n",
    "        'loss_function': 'MultiClass',\n",
    "        \n",
    "        # Ограничим глубину деревьев для ускорения\n",
    "        'depth': 4,\n",
    "        'iterations': 3500,\n",
    "\n",
    "        # Регуляризация и ускорение\n",
    "        'max_bin': 187,\n",
    "        'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
    "        'thread_count': -1,\n",
    "        'bootstrap_type': 'Bernoulli', \n",
    "            \n",
    "        # Важное!\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'auto_class_weights': 'SqrtBalanced',\n",
    "        'early_stopping_rounds': 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://www.dropbox.com/scl/fi/9hb4r3uce0mqz8fkpja17/text_classification_train.csv?rlkey=w42y98wa401gelzou08pp582k&dl=1')\n",
    "test = pd.read_csv('https://www.dropbox.com/scl/fi/7z7rsy14amjeugf166i1t/text_classification_test.csv?rlkey=z53jgwhijd6bpvk7n8n2munwb&dl=1')\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(algorithm,\n",
    "                X,\n",
    "                y,\n",
    "                early_stopping_rounds,\n",
    "                init_params=None,\n",
    "                cat_features=None,\n",
    "                text_features=None,\n",
    "                random_seed=2024\n",
    "    ):\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    print(f\"========= TRAINING {algorithm.__name__} =========\")\n",
    "\n",
    "    for num_fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        if init_params is not None:\n",
    "            model = algorithm(**init_params)\n",
    "        else:\n",
    "            model = algorithm()\n",
    "\n",
    "        if 'CatBoost' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения \n",
    "            train_dataset = Pool(data=X_train, label=y_train, cat_features=cat_features, text_features=text_features)\n",
    "            eval_dataset  = Pool(data=X_eval, label=y_eval, cat_features=cat_features, text_features=text_features)\n",
    "\n",
    "            model.fit(train_dataset,\n",
    "                      eval_set=eval_dataset,\n",
    "                      verbose=0,\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "        elif 'LGBM' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения \n",
    "            train_dataset = Dataset(X_train, y_train, categorical_feature=cat_features, free_raw_data=False,)\n",
    "            eval_dataset  = Dataset(X_eval, y_eval, categorical_feature=cat_features, free_raw_data=False,)\n",
    "\n",
    "            model = lgb.train(params=init_params,\n",
    "                              train_set=train_dataset,\n",
    "                              valid_sets=(eval_dataset),\n",
    "                              #callbacks=[lgb.log_evaluation(10)],\n",
    "                              #           lgb.early_stopping(stopping_rounds=5)],\n",
    "                              categorical_feature=cat_features,\n",
    "                              #verbose_eval=False                   # в новой версии LightGBM по логи по умолчанию отключены\n",
    "                              )\n",
    "\n",
    "        elif 'XGB' in algorithm.__name__:\n",
    "            # Специальный класс для ускорения обучения\n",
    "            train_dataset = xgb.DMatrix(X_train, label=y_train, nthread=-1, enable_categorical=True,)\n",
    "            eval_dataset  = xgb.DMatrix(X_eval,  label=y_eval,  nthread=-1, enable_categorical=True,)\n",
    "\n",
    "            model = xgb.train(params=init_params,\n",
    "                              dtrain=train_dataset,\n",
    "                              evals=[(train_dataset, 'dtrain'), (eval_dataset, 'dtest')],\n",
    "                              verbose_eval=False,\n",
    "                              early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "            X_eval = eval_dataset\n",
    "\n",
    "        # Предсказание на X_eval и расчет RMSE\n",
    "        y_pred = model.predict(X_eval)\n",
    "        score = balanced_accuracy_score(y_eval, y_pred)\n",
    "\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f'FOLD {num_fold}: SCORE {score}')\n",
    "\n",
    "    mean_kfold_score = np.mean(scores, dtype=\"float16\") - np.std(scores, dtype=\"float16\")\n",
    "    print(\"\\nMEAN BALANCED ACCURACY SCORE\", mean_kfold_score)\n",
    "\n",
    "    # Модель с наименьшим значением скора\n",
    "    best_model = models[scores.index(min(scores))]\n",
    "\n",
    "    return mean_kfold_score, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение CatBoost модели с текстовыми признаками и кастомными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_init_params_cust = {\n",
    "        'loss_function': 'MultiClass',\n",
    "        \n",
    "        # Ограничим глубину деревьев для ускорения\n",
    "        'depth': 4,\n",
    "        'iterations': 3500,\n",
    "\n",
    "        # Регуляризация и ускорение\n",
    "        'max_bin': 187,\n",
    "        'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
    "        'thread_count': -1,\n",
    "        'bootstrap_type': 'Bernoulli', \n",
    "            \n",
    "        # Важное!\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'auto_class_weights': 'SqrtBalanced',\n",
    "        'early_stopping_rounds': 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_score, cb_model = train_model(\n",
    "    algorithm=CatBoostClassifier,\n",
    "    X=train.drop(columns=['category']), y=train['category'],\n",
    "    init_params=cb_init_params_cust,\n",
    "    early_stopping_rounds=30,\n",
    "    text_features=['text'],\n",
    "    random_seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pd.DataFrame(cb_model.predict(test), columns=['category']).to_csv('../subs/cb_model_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.7568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shap_result.json' in os.listdir('../src'):\n",
    "    # загрузка параметров из файла\n",
    "    with open('../src/shap_result.json', 'r') as read_file:\n",
    "        shap_result = json.load(read_file)\n",
    "    row = shap_result['shap_result']\n",
    "\n",
    "else:\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(train.drop(columns=['category']), train['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "    model = CatBoostClassifier(**cb_init_params_cust)\n",
    "\n",
    "    train_dataset = Pool(data=X_train, label=y_train, text_features=['text'])\n",
    "    eval_dataset  = Pool(data=X_eval, label=y_eval, text_features=['text'])\n",
    "\n",
    "    model.fit(train_dataset, \n",
    "            eval_set=eval_dataset,\n",
    "            verbose=0, plot=False,\n",
    "            early_stopping_rounds=30)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    cat_features = None\n",
    "\n",
    "    #train_dataset = Pool(data=X_train, label=y_train, cat_features=cat_features, text_features=text_cols)\n",
    "    shap_values = explainer.shap_values(train_dataset)\n",
    "\n",
    "    row = [shap_values[:, feature_ind, :] for feature_ind in range(shap_values.shape[1])]\n",
    "    row = [np.abs(i).mean(0).mean() for i in row]\n",
    "\n",
    "    shap_result={\n",
    "        'shap_result': row\n",
    "        }\n",
    "\n",
    "    # сохранение результатов в файл\n",
    "    with open('../src/shap_result.json', 'w') as f:\n",
    "        json.dump(shap_result, f)\n",
    "\n",
    "top_shap_idx = sorted(range(len(row)), key=lambda k: row[k], reverse=True)\n",
    "\n",
    "df_plot = []\n",
    "cum_shap_value = 0\n",
    "for shap_value in sorted(row, reverse=True):\n",
    "    cum_shap_value += shap_value\n",
    "    df_plot.append(cum_shap_value)\n",
    "df_plot = pd.DataFrame(df_plot, columns=['shap_value'])\n",
    "\n",
    "sns.relplot(\n",
    "    data=df_plot,\n",
    "    kind=\"line\",\n",
    "    height=4, \n",
    "    aspect=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор количества фичей\n",
    "for num_col in range(200, 900, 50):\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(train[train.drop(columns=['category']).columns[top_shap_idx[:num_col]]], train['category'], test_size=0.2, random_state=42)\n",
    "    train_dataset = Pool(data=X_train, label=y_train, text_features=['text'])\n",
    "    eval_dataset  = Pool(data=X_eval, label=y_eval, text_features=['text'])\n",
    "\n",
    "    cb_model = CatBoostClassifier(**cb_init_params_cust)\n",
    "    cb_model.fit(train_dataset, \n",
    "            eval_set=eval_dataset,\n",
    "            verbose=0, plot=False, \n",
    "            early_stopping_rounds=30)\n",
    "\n",
    "    y_pred = cb_model.predict(X_eval)\n",
    "    score = balanced_accuracy_score(y_eval, y_pred)\n",
    "    print(num_col, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_score, cb_model = train_model(\n",
    "    algorithm=CatBoostClassifier,\n",
    "    X=train[train.drop(columns=['category']).columns[top_shap_idx[:500]]], y=train['category'],\n",
    "    init_params=cb_init_params_cust,\n",
    "    early_stopping_rounds=30,\n",
    "    text_features=['text'],\n",
    "    random_seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pd.DataFrame(cb_model.predict(test[test.columns[top_shap_idx[:500]]]),\n",
    "             columns=['category']).to_csv('../subs/cb_model_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.7604"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_TFIDF = vectorizer.fit_transform(train['text'])\n",
    "X_TFIDF_train, X_TFIDF_eval, y_TFIDF_train, y_TFIDF_eval = train_test_split(X_TFIDF, train['category'], test_size=0.2, random_state=42)\n",
    "X_TFIDF_test  = vectorizer.transform(test['text'])\n",
    "\n",
    "print(X_TFIDF_train.shape)\n",
    "print(X_TFIDF_eval.shape)\n",
    "print(X_TFIDF_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обечение с валидацией\n",
    "logreg = LogisticRegression(random_state=RANDOM_STATE)\n",
    "logreg.fit(X_TFIDF_train, y_TFIDF_train)\n",
    "print(balanced_accuracy_score(y_TFIDF_eval, logreg.predict(X_TFIDF_eval)))\n",
    "\n",
    "# Обучение на всех данных\n",
    "logreg.fit(X_TFIDF, train['category'])\n",
    "pd.DataFrame(logreg.predict(X_TFIDF_test), columns=['category']).to_csv('../subs/tfids_logreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров\n",
    "def objective_svc(trial):\n",
    "\n",
    "    C = trial.suggest_float(\"C\",0.1,1000)\n",
    "    gamma = trial.suggest_float(\"gamma\",0.0001,1)\n",
    "    kernel = trial.suggest_categorical(\"kernel\",['rbf','poly']) \n",
    "    model = SVC(\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        kernel=kernel,\n",
    "        random_state=RANDOM_STATE\n",
    "    )  \n",
    "    score = cross_val_score(model, X_TFIDF_train, y, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "if 'params_svc.json' in os.listdir('../src'):\n",
    "    # загрузка параметров из файла\n",
    "    with open('../src/params_svc.json', 'r') as read_file:\n",
    "        params_svc = json.load(read_file)\n",
    "        \n",
    "else:\n",
    "    X_TFIDF_train = vectorizer.fit_transform(train['text'])\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective_svc,\n",
    "                n_trials=100,\n",
    "                n_jobs = -1)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    params_svc = trial.params\n",
    "\n",
    "    # сохранение результатов в файл\n",
    "    with open('../src/params_svc.json', 'w') as f:\n",
    "        json.dump(params_svc, f)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in params_svc.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обечение с валидацией\n",
    "#svc_clf = SVC(C=2.5, gamma=0.7, random_state=RANDOM_STATE)\n",
    "svc_clf = SVC(**params_svc)\n",
    "svc_clf.fit(X_TFIDF_train, y_TFIDF_train)\n",
    "print(balanced_accuracy_score(y_TFIDF_eval, svc_clf.predict(X_TFIDF_eval)))\n",
    "\n",
    "# Обучение на всех данных\n",
    "svc_clf.fit(X_TFIDF, train['category'])\n",
    "pd.DataFrame(svc_clf.predict(X_TFIDF_test), columns=['category']).to_csv('../subs/tfids_svc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде  0.7716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обечение с валидацией\n",
    "linear_svc = LinearSVC(tol=0.01, dual=True, C=0.6, random_state=RANDOM_STATE)\n",
    "linear_svc.fit(X_TFIDF_train, y_TFIDF_train)\n",
    "print(balanced_accuracy_score(y_TFIDF_eval, linear_svc.predict(X_TFIDF_eval)))\n",
    "\n",
    "# Обучение на всех данных\n",
    "linear_svc.fit(X_TFIDF, train['category'])\n",
    "pd.DataFrame(linear_svc.predict(X_TFIDF_test), columns=['category']).to_csv('../subs/tfids_lin_svc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обечение с валидацией\n",
    "et_clf = ExtraTreesClassifier(n_estimators = 6_000, max_depth = 8, min_samples_leaf = 2, bootstrap = True,\n",
    "                              class_weight = 'balanced',random_state = RANDOM_STATE, verbose=False, n_jobs=-1)\n",
    "et_clf.fit(X_TFIDF_train, y_TFIDF_train)\n",
    "print(balanced_accuracy_score(y_TFIDF_eval, et_clf.predict(X_TFIDF_eval)))\n",
    "\n",
    "# Обучение на всех данных\n",
    "et_clf.fit(X_TFIDF, train['category'])\n",
    "pd.DataFrame(et_clf.predict(X_TFIDF_test), columns=['category']).to_csv('../subs/tfids_et.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.6836 (можно подбором гиперпараметров увеличить до 0.7432, но метрика при стекинге падает)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обечение с валидацией\n",
    "rf = RandomForestClassifier(n_estimators=10_000, max_depth=200, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "rf.fit(X_TFIDF_train, y_TFIDF_train)\n",
    "print(balanced_accuracy_score(y_TFIDF_eval, rf.predict(X_TFIDF_eval)))\n",
    "\n",
    "# Обучение на всех данных\n",
    "rf.fit(X_TFIDF, train['category'])\n",
    "pd.DataFrame(rf.predict(X_TFIDF_test), columns=['category']).to_csv('../subs/tfids_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy на лидерборде 0.7068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стекинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker:\n",
    "    def __init__(self, base_models, meta_model, preprocessing=None, metafeatures_mode=None):\n",
    "        \"\"\"\n",
    "        base_models - список базовых моделей, которые нужно обучать на изначальных данных\n",
    "        meta_model - мета модель, которая обучается на предсказаниях базовых моделей\n",
    "        metafeatures_mode - режим формирования фичей ('pred' - предикт, 'proba' - вероятность, 'log_proba' - логарифм вероятности\n",
    "        preprocessing - список словарей операций над датасетом:\n",
    "            col_select - выбор столбцов\n",
    "            col_drop - удаление столбцов\n",
    "            tfidf - преобразование tfidf по указанному столбцу\n",
    "            text_features - список текстовых сболбцов для подачи в CatBoost\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.preprocessing = preprocessing\n",
    "        self.metafeatures_mode = metafeatures_mode if metafeatures_mode else [['pred'] for m in base_models]\n",
    "        self.vectorizer = None\n",
    "        self.additional_meta_features = False # дополнительные метафичи (разность, деление существующих)\n",
    "        self.meta_cat_features = None\n",
    "        self.meta_le = {}\n",
    "\n",
    "    def X_preprocessing(self, X, num_model):\n",
    "        if 'col_drop' in self.preprocessing[num_model]:\n",
    "            X.drop(columns=self.preprocessing[num_model]['col_drop'], inplace=True)\n",
    "        if 'col_select' in self.preprocessing[num_model]:\n",
    "            X = X[self.preprocessing[num_model]['col_select']] \n",
    "        if 'tfidf' in self.preprocessing[num_model]:\n",
    "            X = self.vectorizer.transform(X[self.preprocessing[num_model]['tfidf']])      \n",
    "        return X\n",
    "    \n",
    "    def base_model_one_pred(self, model, mode, X):\n",
    "        if mode == 'pred':\n",
    "            preds = model.predict(X)\n",
    "            preds = preds.reshape(len(preds), 1)\n",
    "        elif mode == 'proba':\n",
    "            if model.__class__.__name__ == 'LinearSVC':\n",
    "                 preds = model._predict_proba_lr(X)\n",
    "            else:\n",
    "                preds = model.predict_proba(X)\n",
    "        elif mode == 'log_proba':\n",
    "            preds = model.predict_log_proba(X)\n",
    "        \n",
    "        # костыль для избавления от -inf в RF\n",
    "        if model.__class__.__name__ == 'RandomForestClassifier':\n",
    "            preds[np.where(preds == float('-inf'))] = -100\n",
    "            preds[np.where(preds == float('inf'))] = 100\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def base_model_all_preds(self, model, num_model, X):\n",
    "        #preds_all = None\n",
    "        preds_all = np.empty((X.shape[0], 0))\n",
    "        for mode in self.metafeatures_mode[num_model]:\n",
    "            preds = self.base_model_one_pred(model, mode, X)\n",
    "            #preds_all = preds if preds_all is None else np.concatenate([preds_all, preds], axis=1) \n",
    "            preds_all = np.concatenate([preds_all, preds], axis=1)\n",
    "        return preds_all\n",
    "    \n",
    "    def base_model_fit(self, model, num_model, X_train, y_train, X_val=None, y_val=None):\n",
    "                \n",
    "        if model.__class__.__name__ == 'CatBoostClassifier':\n",
    "            #best_iter = 0\n",
    "            text_features = []\n",
    "            if 'text_features' in self.preprocessing[num_model]:            # возможно вынести наверх (в основную функцию)\n",
    "                for i in preprocessing[num_model]['text_features']:\n",
    "                    text_features.append(X_train.columns.get_loc(i))\n",
    "            cat_features = []\n",
    "            for i in range(X_train.shape[1]):\n",
    "                if type(X_train.iloc[0, i]) == str:\n",
    "                    if i not in text_features:\n",
    "                        cat_features.append(i)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                model.fit(X_train, y_train, eval_set=(X_val, y_val), cat_features=cat_features, text_features=text_features,\n",
    "                          verbose=False, early_stopping_rounds=30)\n",
    "                #if model.best_iteration_ > best_iter:\n",
    "                    #best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model.fit(X_train, y_train, cat_features=cat_features, text_features=text_features, verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "    def fit_base(self, X, y, n_fold=5):\n",
    "        # если есть tfidf - обучаем vectorizer\n",
    "        for params in preprocessing:\n",
    "            if 'tfidf' in params.keys():\n",
    "                self.vectorizer = TfidfVectorizer()\n",
    "                self.vectorizer.fit(X[params['tfidf']])\n",
    "                print('Сформирован vectorizer:', self.vectorizer.get_feature_names_out().shape[0], 'токенов.')\n",
    "                break\n",
    "        \n",
    "        folds = KFold(n_splits=n_fold)\n",
    "        final_features = np.empty((len(X), 0))\n",
    "\n",
    "        for num_model, model in enumerate(self.base_models):\n",
    "            \n",
    "            preds_model = None\n",
    "        \n",
    "            for train_indices, val_indices in folds.split(X, y):\n",
    " \n",
    "                X_train, X_val = X.loc[train_indices], X.loc[val_indices]\n",
    "                X_train, X_val = self.X_preprocessing(X_train, num_model), self.X_preprocessing(X_val, num_model)\n",
    "                y_train, y_val = y[train_indices], y[val_indices]        \n",
    "            \n",
    "                self.base_model_fit(model, num_model, X_train, y_train, X_val, y_val)\n",
    "                preds_fold = self.base_model_all_preds(model, num_model, X_val)\n",
    "                preds_model = preds_fold if preds_model is None else np.concatenate([preds_model, preds_fold], axis=0)\n",
    "            \n",
    "            final_features = np.concatenate([final_features, preds_model], axis=1)\n",
    "\n",
    "            self.base_model_fit(model, num_model, self.X_preprocessing(X, num_model), y)\n",
    "            print('Обучена базовая модель №', num_model+1, model.__class__.__name__)\n",
    "            \n",
    "        return final_features\n",
    "    \n",
    "    def add_meta_features(self, X):\n",
    "        \n",
    "        meta_num_features = []\n",
    "        for i in range(X.shape[1]):\n",
    "            if type(X[0][i]) == float:\n",
    "                meta_num_features.append(i)\n",
    "        \n",
    "        new_features = []\n",
    "        for source in meta_num_features:\n",
    "            for destination in meta_num_features:\n",
    "                row = X[:, source] / X[:, destination]\n",
    "                new_features.append(row.reshape(len(row),1))\n",
    "                row = X[:, source] - X[:, destination]\n",
    "                new_features.append(row.reshape(len(row),1))\n",
    "        new_features = np.concatenate(new_features, axis=1)\n",
    "        new_features = np.concatenate([X, new_features], axis=1)\n",
    "\n",
    "        return new_features\n",
    "\n",
    "    def fit_meta(self, meta_features, y):\n",
    "        \n",
    "        self.meta_cat_features = []\n",
    "        for i in range(meta_features.shape[1]):\n",
    "            if type(meta_features[0][i]) == str:\n",
    "                self.meta_cat_features.append(i)\n",
    "        \n",
    "        if self.meta_model.__class__.__name__ == 'CatBoostClassifier':\n",
    "            self.meta_model.fit(pd.DataFrame(meta_features), y, cat_features=self.meta_cat_features, verbose=False)\n",
    "        else:\n",
    "            for col in self.meta_cat_features:\n",
    "                self.meta_le[col] = LabelEncoder()\n",
    "                meta_features[:, col] = self.meta_le[col].fit_transform(meta_features[:, col])\n",
    "\n",
    "            self.meta_model.fit(pd.DataFrame(meta_features), y)\n",
    "        print('cat_features', self.meta_cat_features)\n",
    "        print('Обучена метамодель', self.meta_model.__class__.__name__)\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        meta_features = self.fit_base(X, y)\n",
    "        if self.additional_meta_features == True:\n",
    "            meta_features=self.add_meta_features(meta_features)\n",
    "            print('Сформированы дополнительные фичи')\n",
    "        self.fit_meta(meta_features, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        final_features = np.empty((len(X), 0))\n",
    "        \n",
    "        for num_model, model in enumerate(self.base_models):\n",
    "            \n",
    "            # X preprocessing\n",
    "            X_test = self.X_preprocessing(X, num_model)\n",
    "            preds_model = self.base_model_all_preds(model, num_model, X_test)\n",
    "            final_features = np.concatenate([final_features, preds_model], axis=1)\n",
    "        \n",
    "        \n",
    "        if self.meta_model.__class__.__name__ == 'CatBoostClassifier':\n",
    "            final_preds = self.meta_model.predict(final_features)\n",
    "        else:\n",
    "            for col in self.meta_cat_features:\n",
    "                final_features[:, col] = self.meta_le[col].transform(final_features[:, col])\n",
    "\n",
    "            final_preds = self.meta_model.predict(final_features)\n",
    "            final_preds = final_preds.reshape(len(final_preds), 1)\n",
    "\n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем экземпляр Stacker\n",
    "base_models = [CatBoostClassifier(**cb_init_params_cust),\n",
    "               LogisticRegression(random_state=RANDOM_STATE),\n",
    "               SVC(**params_svc, probability=True, decision_function_shape='ovr', random_state=RANDOM_STATE),\n",
    "               LinearSVC(tol=0.01, dual=True, C=0.6, random_state=RANDOM_STATE),\n",
    "               ExtraTreesClassifier(n_estimators = 6_000, max_depth = 8, min_samples_leaf = 2, bootstrap = True,\n",
    "                                    class_weight = 'balanced',random_state = RANDOM_STATE, verbose=False, n_jobs=-1,),\n",
    "               #RandomForestClassifier(n_estimators = 10_000, max_depth = 200, n_jobs=-1, random_state = RANDOM_STATE)\n",
    "               ]\n",
    "\n",
    "meta_model = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "preprocessing = [{'col_select':train.drop(columns=['category']).columns[top_shap_idx[:500]], 'text_features':['text']},\n",
    "                 {'col_select':['text'], 'tfidf':'text'},\n",
    "                 {'col_select':['text'], 'tfidf':'text'},\n",
    "                 {'col_select':['text'], 'tfidf':'text'},\n",
    "                 {'col_select':['text'], 'tfidf':'text'},\n",
    "                 #{'col_select':['text'], 'tfidf':'text'}\n",
    "                 ]\n",
    "\n",
    "metafeatures_mode = [['pred', 'proba', 'log_proba'],\n",
    "                     ['pred', 'proba', 'log_proba'],\n",
    "                     ['pred', 'proba', 'log_proba'],\n",
    "                     ['pred', 'proba'],\n",
    "                     ['pred', 'proba', 'log_proba'],\n",
    "                     #['pred', 'proba', 'log_proba']\n",
    "                     ]\n",
    "\n",
    "stacker = Stacker(base_models, meta_model, preprocessing, metafeatures_mode)\n",
    "stacker.fit(train.drop(columns=['category']), train['category'])\n",
    "res = stacker.predict(test)\n",
    "pd.DataFrame(res, columns=['category']).to_csv('../subs/tfids_stacking.csv', index=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
